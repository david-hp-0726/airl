## Implementation Overview

In this project, the **Adversarial Inverse Reinforcement Learning (AIRL)** method from Fu et al. (ICLR 2018) [(paper)](https://arxiv.org/abs/1710.11248) is reproduced. The pipeline consists of three main stages:

1. **Expert Training**  
   I train expert policies in four continuous-control environments—Pendulum-v1, Swimmer-v4, HalfCheetah-v4, and Ant-v4—using Trust Region Policy Optimization (TRPO) on the true reward until performance plateaus.

2. **Expert Trajectory Collection**  
   Experts are rolled out to collect demonstration datasets of $(s, a, s')$ tuples. These expert trajectories are saved the AIRL training phase.

3. **AIRL Training**  
   The AIRL discriminator learns two functions:
   - **Reward network** $$g_\theta(s)$$: a 2-layer MLP (256 hidden units, ReLU).
   - **Shaping network** $$h_\phi(s)$$: a 2-layer MLP (256 hidden units, ReLU).

   At each iteration, the discriminator computes
   $$f_{\theta,\phi}(s, a, s') = g_\theta(s) + \gamma\,h_\phi(s') - h_\phi(s),$$
   and is trained to distinguish expert transitions from those generated by the current policy. The learned reward defined in the paper is
   $$r(s,a,s') = \log D_{\theta,\phi}(s,a,s') - \log\bigl(1 - D_{\theta,\phi}(s,a,s')\bigr),$$
   and update the policy with TRPO to maximize this learned reward. This adversarial loop continues until the recovered policy replicates expert behavior across all four environments.

   On the policy side, optimize a stochastic policy $\pi_\omega(a \mid s)$ using the Actor-Critic algorithm to maximize the objective function $E_{\pi_\omega}[\nabla_\omega \log{\pi(a|s)}\cdot\hat{A}(s,a)]$. The policy uses:
   - **Actor**: a 2-layer MLP with 256 hidden units (tanh activations) to learn action means and log standard deviation.
   - **Critic**: a learnable value function $V_\psi(s)$ (same architecture) trained using the TD method.
